{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - NLP Pipeline\n",
    "\n",
    "![NLP Pipeline](Images/NLPPipeline.jpg)\n",
    "\n",
    "1. Data acquisition : Collect data relevant to the given task\n",
    "2. Text cleaning\n",
    "3. Pre-processing : Data is converted into a canonical form\n",
    "4. Feature engineering : we carve out indicators that are most suitable for the task at hand. These indicators are converted into a format that is understandable by modeling algorithms\n",
    "5. Modeling\n",
    "6. Evaluation\n",
    "7. Deployment\n",
    "8. Monitoring and model updating\n",
    "\n",
    "Note that, in the real world, the process may not always be linear as it’s shown in the pipeline in Figure 2-1; it often involves going back and forth between individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data acquisition\n",
    "\n",
    "Strategies for gathering relevant data for a NLP project (in less-than-ideal scenario):\n",
    "\n",
    "1. If we have **little or no data, we can start by looking at patterns in the data that indicate if the incoming message is a sales or support query**. We can then use regular expressions and other heuristics to match these patterns to separate sales queries from support queries. We evaluate this solution by collecting a set of queries from both categories and calculating what percentage of the messages were correctly identified by our system.\n",
    "\n",
    "\n",
    "2. Now we can start thinking about using NLP techniques. For this, we need labeled data\n",
    "\n",
    "    * Use public dataset\n",
    "\n",
    "    * Scrape data\n",
    "\n",
    "    * Product intervention (better instrumentation collecting data)\n",
    "\n",
    "    * Data augmentation : NLP has a bunch of techniques through which we can take a small dataset and use\n",
    "    some tricks to create more data.\n",
    "\n",
    "        - Synonym replacement (Synsets or Wordnet)\n",
    "\n",
    "        - Back translation : we use a translation tool to translate in another language and translate back to the initial language to get another sentence\n",
    "\n",
    "        - TF-IDF-bases worlds replacement : Back translation can lose certain words that are crucial to the sentence. A concept we’ll introduce in Chapter 3.\n",
    "\n",
    "        - Bigram flipping : Divide the sentence into bigrams. Take one bigram at random and flip it. For example: “I am going to the supermarket.” Here, we take the bigram “going to” and replace it with the flipped one: “to going.”\n",
    "\n",
    "        - Replacing entities : Replace entities like person name, location, organization, etc., with other entities in the same category.\n",
    "\n",
    "        - Adding noise to data : In many NLP applications, the incoming data contains spelling mistakes.For example, randomly choose a word in a sentence and replace it with another word that’s closer in spelling to the first word. For the \"fat finger\" problem, simulate a QWERTY keyboard error by replacing a few characters with their neighboring characters on the QWERTY keyboard.\n",
    "\n",
    "        - Snorkel : This is a system for building training data automatically, without manual labeling. Using Snorkel, a large training dataset can be “created”—without manual labeling—using heuristics and creating synthetic data by transforming existing data and creating new data samples.\n",
    "\n",
    "        - Easy Data Augmentation (EDA) and NLPAug : These two libraries are used to create synthetic samples for NLP. They provide implementation of various data augmentation techniques, including some techniques that we discussed previously.\n",
    "\n",
    "        - Active learning : In such cases, the question becomes: for which data points should we ask for labels to maximize learning while keeping the labeling cost low?\n",
    "\n",
    "\n",
    "In order for most of the techniques we discussed in this section to work well, one key requirement is a clean dataset to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Extraction and Cleanup\n",
    "\n",
    "**Definition**: process of extracting raw text from the input data by removing all the other non-textual information, such as markup, metadata, etc., and converting the text to the required encoding format.\n",
    "(From PDF, HTML, text, datastream)\n",
    "\n",
    "### HTML Parsing and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import ssl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://stackoverflow.com/questions/415511/%5Chow-to-get-the-current-time-in-python\"\n",
    "\n",
    "# In order not to check the SSL certificate\n",
    "context = ssl._create_unverified_context()\n",
    "html = urlopen(url, context=context).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html ,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  What is the module/method used to get the current time?\n",
      "Answer :  Use:\n",
      ">>> import datetime\n",
      ">>> datetime.datetime.now()\n",
      "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now())\n",
      "2009-01-06 15:08:24.789150\n",
      "\n",
      "And just the time:\n",
      ">>> datetime.datetime.now().time()\n",
      "datetime.time(15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now().time())\n",
      "15:08:24.789150\n",
      "\n",
      "See the documentation for more information.\n",
      "To save typing, you can import the datetime object from the datetime module:\n",
      ">>> from datetime import datetime\n",
      "\n",
      "Then remove the leading datetime. from all of the above.\n"
     ]
    }
   ],
   "source": [
    "question = soup.find(\"div\", {\"class\": \"question\"}) \\\n",
    "            .find(\"div\", {\"class\": \"s-prose js-post-body\"}).get_text().strip()\n",
    "print(\"Question : \", question)\n",
    "\n",
    "answer = soup.find(\"div\", {\"class\": \"answer js-answer accepted-answer\"})\\\n",
    "        .find(\"div\", {\"class\":\"s-prose js-post-body\"}).get_text().strip()\n",
    "print(\"Answer : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Normalization\n",
    "\n",
    "*As we develop code for cleaning up HTML tags, we may also encounter various Unicode characters, including symbols, emojis, and other graphic characters*\n",
    "\n",
    "To parse such non-textual symbols and special characters, we use Unicode normalization.\n",
    "\n",
    "*Example*:\n",
    "   \n",
    "    text = ’I love ! Shall we book a to gizza?’\n",
    "    \n",
    "    Text = text.encode(\"utf-8\")\n",
    "    \n",
    "    print(Text)\n",
    "\n",
    "    which outputs:\n",
    "    \n",
    "    b'I love Pizza \\xf0\\x9f\\x8d\\x95! Shall we book a cab \\xf0\\x9f\\x9a\\x95\n",
    "    to get pizza?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction\n",
    "\n",
    "Shorthand typing, fat finger problem ...\n",
    "\n",
    "We can use the Microsoft REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_type\": \"SpellCheck\",\n",
      "    \"flaggedTokens\": [\n",
      "        {\n",
      "            \"offset\": 0,\n",
      "            \"token\": \"Bnjour\",\n",
      "            \"type\": \"UnknownToken\",\n",
      "            \"suggestions\": [\n",
      "                {\n",
      "                    \"suggestion\": \"Bonjour\",\n",
      "                    \"score\": 0.7710960127696465\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# URL et clé API\n",
    "endpoint = \"https://api.bing.microsoft.com/v7.0/SpellCheck\"\n",
    "api_key = \"e187bcc54d7c41c2bbf92ba12923ae86\"\n",
    "\n",
    "# Paramétrage API\n",
    "example_text = \"Bnjour à tos\" # the text to be spell-checked\n",
    "data = {'text': example_text}\n",
    "\n",
    "params = {\n",
    "            'mkt':'en-us',\n",
    "            'mode':'proof',\n",
    "        }\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "\n",
    "# Demande\n",
    "try:\n",
    "    response = requests.post(endpoint, headers=headers, params=params, data=data)\n",
    "    json_response = response.json()\n",
    "    print(json.dumps(json_response, indent=4))\n",
    "except Exception as ex:\n",
    "    raise ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going beyond APIs, we can build our own spell checker using a huge dictionary of words from a specific language.\n",
    "\n",
    "    *Library*: pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System-Specific Error Correction\n",
    "\n",
    "* **PDF Documents** : The pipeline in this case starts with extraction of plain text from PDF documents. However, different PDF documents are encoded differently, and sometimes, we may not be able to extract the full text, or the structure of the text may get messed up.\n",
    "\n",
    "    *Several libraries*: PyPDF, PDFMiner\n",
    "\n",
    "\n",
    "* **Scanned documents**:\n",
    "    *Several libraries*: Tesseract\n",
    "\n",
    "![Snippet OCR](Images/SnippetOCR.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the nineteenth century the only Kind of linguistics considered\n",
      "seriously was this comparative and historical study of words in languages\n",
      "known ot believed to be cognate—say the Semitic languages, or the Indo-\n",
      "European languages. It is significant that the Germans who really made\n",
      "the subject what it was, used the term Indo-germanisch. Those who know\n",
      "the popular works of Otto Jespersen will remember how firmly he\n",
      "declares that linguistic science is histotical. And those who have noticed\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string, pytesseract\n",
    "\n",
    "filename = r\"Images/SnippetOCR.jpg\"\n",
    "\n",
    "pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "# You need to install tesseract\n",
    "\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All NLP software typically works at the sentence level and expects a separation of words at the minimum.\n",
    "So, we need some way to :\n",
    "* split a text into words and sentences before proceeding further in a processing pipeline.\n",
    "* Sometimes, we need to remove special characters and digits\n",
    "* lower-case / upper-case\n",
    "\n",
    "**Here are some common preprocessing steps used in NLP software:**\n",
    "\n",
    "* Preliminaries: Sentence segmentation and word tokenization.\n",
    "* Frequent steps : Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc.\n",
    "* Other steps: Normalization, language detection, code mixing, transliteration, etc.\n",
    "* Advanced processing: POS tagging, parsing, coreference resolution, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "* **Sentence segmentation** : Splitting the text into sentences.\n",
    "    Risks of abbreviations, forms of addresses (Dr., Mr., etc.), or ellipses (...) that may break the simple rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"\"\"In the previous chapter, we saw examples of some common NLP\n",
    "applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our\n",
    "organization. We would normally walk through the requirements and break the\n",
    "problem down into several sub-problems, then try to develop a step-by-step\n",
    "procedure to solve them. Since language processing is involved, we would also\n",
    "list all the forms of text processing needed at each step. This step-by-step\n",
    "processing of text is known as pipeline. It is the series of steps involved in\n",
    "building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures\n",
    "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
    "in the workplace. Laying out and developing a text-processing pipeline is seen\n",
    "as a starting point for any NLP application development process. In this\n",
    "chapter, we will learn about the various steps involved and how they play\n",
    "important roles in solving the NLP problem and we’ll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we’ll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the previous chapter, we saw examples of some common NLP\\napplications that we might encounter in everyday life.', 'If we were asked to\\nbuild such an application, think about how we would approach doing so at our\\norganization.', 'We would normally walk through the requirements and break the\\nproblem down into several sub-problems, then try to develop a step-by-step\\nprocedure to solve them.', 'Since language processing is involved, we would also\\nlist all the forms of text processing needed at each step.', 'This step-by-step\\nprocessing of text is known as pipeline.', 'It is the series of steps involved in\\nbuilding any NLP model.', 'These steps are common in every NLP project, so it\\nmakes sense to study them in this chapter.', 'Understanding some common procedures\\nin any NLP pipeline will enable us to get started on any NLP problem encountered\\nin the workplace.', 'Laying out and developing a text-processing pipeline is seen\\nas a starting point for any NLP application development process.', 'In this\\nchapter, we will learn about the various steps involved and how they play\\nimportant roles in solving the NLP problem and we’ll see a few guidelines\\nabout when and how to use which step.', 'In later chapters, we’ll discuss\\nspecific pipelines for various NLP tasks (e.g., Chapters 4–7).']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Work tokenization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to sentence tokenization, to tokenize a sentence into words, we can start with a simple rule to split text into words based on the presence of punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.', 'If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.', 'We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.', 'Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.', 'This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.', 'It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.', 'These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.', 'Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.', 'Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.', 'In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '’', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.', 'In', 'later', 'chapters', ',', 'we', '’', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4–7', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "\n",
    "* \"\"Mr. Jack O’Neil works at Melitas Marg, located at 245 Yonge Avenue, Austin, 70272.\" If we run this through the NLTK tokenizer, O, ‘, and Neil are identified as three separate tokens.\n",
    "\n",
    "* \"There are \\$10,000 and €1000 which are there just for testing a tokenizer\" through this tokenizer, while $ and 10,000 are identified as separate tokens, €1000 is identified as a single token.\n",
    "\n",
    "* If we want to tokenize tweets, this tokenizer will separate a hashtag into two tokens: a “#” sign and the string that follows it. (A point to note in this context is that NLTK also has a tweet tokenizer)\n",
    "\n",
    "* “N.Y.!” has a total of three punctuations. But in English, N.Y. stands for New York, hence “N.Y.” should be treated as a single word\n",
    "\n",
    "> Such language-specific exceptions can be specified in the tokenizer provided by spaCy\n",
    "\n",
    "![Language-Specific exceptions during tokenization](Images/tokenization.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Steps\n",
    "\n",
    "*Say we’re designing software that identifies the category of a news article as one of politics, sports, business, and other.*\n",
    "\n",
    "1. Assume we have a good sentence segmenter and word tokenizer in place.\n",
    "2. **Removing stop words**: At that point, we would have to start thinking about what kind of information is useful for developing a categorization tool. Some of the frequently used words in English, such as a, an, the, of, in, etc., are not particularly useful for this task, as they don’t carry any content on their own to separate between the four categories. --> Such words are called stop words and are typically (though not always) removed from further analysis in such problem scenarios\n",
    "\n",
    "3. **Removing punctuation and/or numbers**\n",
    "4. **All text lowercased**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cokev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(text):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        return  [token.lower() for token in tokens if token not in mystopworkds \\\n",
    "                and not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text) for text in texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming : refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., “car” and “cars” are both reduced to “car”).\n",
    "\n",
    "*Popular stemming algorithm called Porter Stemmer using NLTK*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car revolut\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word1, word2 = \"cars\", \"revolution\"\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatization : of mapping all the different forms of a word to its base word, or lemma.\n",
    "\n",
    "    For example, the adjective “better,” when stemmed, remains the same. However, upon lemmatization, this should become \"good\".\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cokev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better well\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "token = sp(u\"better\")\n",
    "for word in token: print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We typically lowercase the text before stemming.\n",
    "\n",
    "* We also don’t remove tokens or lowercase the text before doing lemmatization because we have to know the part of speech of the word to get its lemma, and that requires all tokens in the sentence to be intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pre-Processing Steps\n",
    "\n",
    "While we haven’t explicitly stated the nature of the texts, we have assumed that we’re dealing with regular English text. What’s different if that’s not the case?\n",
    "\n",
    "* Text normalization : scenario where we’re working with a collection of social media posts to detect news events. -> It’s useful to reach a canonical representation of text that captures all these variations into one representation\n",
    "\n",
    "    * Lowercase/Uppercase\n",
    "    * Converts digits to text (9 to nine)\n",
    "    * Expand abbreviations\n",
    "    * Dictionnary of different sellings of a preset collection of words mapped to a single spelling\n",
    "\n",
    "* Language detection : Non-english reviews for example. Language detection is performed as the first step in an NLP pipeline\n",
    "\n",
    "    *Libraries like Polyglot allow language detection.* \n",
    "    Once this step is done, the next steps could follow a language-specific pipeline.\n",
    "\n",
    "* Code mixing and transliteration : The same phrase with many different languages.\n",
    "\n",
    "    * Code mixing : phenomenon of switching between languages.\n",
    "\n",
    "    * Transliteration : people use multiple languages in their write-ups, they often type words from these languages in Roman script, with English spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Processing\n",
    "\n",
    "Imagine we’re asked to develop a system to identify person and organization names in our company’s collection of one million documents. --> We use **POS tagging**.\n",
    "\n",
    "*Libraries like NLTK, Spacy and Parsey McParseFace Tagger* allow us to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Charles PROPN Xxxxx True False\n",
      "Spencer Spencer PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "was be AUX xxx True True\n",
      "born bear VERB xxxx True False\n",
      "on on ADP xx True True\n",
      "16 16 NUM dd False False\n",
      "April April PROPN Xxxxx True False\n",
      "1889 1889 NUM dddd False False\n",
      "to to ADP xx True True\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "                          SPACE      False False\n",
      "( ( PUNCT ( False False\n",
      "born bear VERB xxxx True False\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Harriet Harriet PROPN Xxxxx True False\n",
      "Pedlingham Pedlingham PROPN Xxxxx True False\n",
      "Hill Hill PROPN Xxxx True False\n",
      ") ) PUNCT ) False False\n",
      "and and CCONJ xxx True True\n",
      "Charles Charles PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "Sr Sr PROPN Xx True False\n",
      "' ' PUNCT ' False False\n",
      ") ) PUNCT ) False False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin \\\n",
    "            (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr')\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation extraction: along with identifying person and organization names in our company’s collection of one million documents, we’re also asked to identify if a given person and organization are related to each other in some way. Think about what kind of pre-processing we need for this case:\n",
    "\n",
    "1. POS tagging\n",
    "\n",
    "2. A way of identifying person and organization names, which is a separate information extraction task known as named entity recognition (NER)\n",
    "\n",
    "3. Identify patterns indicating “relation” between two entities in a sentence (using syntactic representation of the sentence, such as parsing)\n",
    "\n",
    "4. To identify and link multiple mentions of an entity : coreference resolution\n",
    "\n",
    "![Output of different stages of NLP pipeline processing](Images/OutputPipeline.jpg)\n",
    "\n",
    "\n",
    "![Pre-processing steps](Images/preprocessingsteps.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**Definition** (feature engineering, feature etraction, text representation) : way to feed this pre-processed text into an ML algorithm.\n",
    "\n",
    "**2 approaches**:\n",
    "\n",
    "1. Classical NLP and traditional ML pipeline :\n",
    "\n",
    "    Convert the raw data into a format that can be consumed by a machine : count the number of positive/negative words.\n",
    "    Features are heavily inspired by the task at hand as well as domain knowledge.\n",
    "\n",
    "    > One of the advantages of handcrafted features is that the model remains interpretable.\n",
    "\n",
    "    The main drawback of classical ML models is the feature engineering. Handcrafted feature engineering becomes a bottleneck for both model performance and the model development cycle.\n",
    "\n",
    "2. DL Pipeline : \n",
    "\n",
    "    In the DL pipeline, the raw data (after preprocessing) is directly fed to a model. The model is capable of “learning” features from the data.\n",
    "\n",
    "    > The model loses interpretability.\n",
    "\n",
    "![Feature engineering](Images/FeatureEngineering.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "1. At the start, when we have limited data, we can use simpler methods and rules. \n",
    "2. Over time, with more data and a better understanding of the problem, we can add more complexity and improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with Simple Heuristics\n",
    "\n",
    "* For instance, in email spam-classification tasks, we may have a blacklist of domains that are used exclusively to send spam.\n",
    "\n",
    "* In an e-commerce setting, we may use a heuristic based on the number of purchases for ordering search results and show products belonging to the same category as recommendations.\n",
    "\n",
    "* Using regular expressions:\n",
    "\n",
    "    * Normal regular expressions : for some information, such as email IDs, dates, and telephone numbers.\n",
    "    \n",
    "    * Stanford NLP’s TokensRegex and spaCy’s rule-based matching: The following image shows a pattern that looks for text containing the lemma “match,” appearing as a noun, optionally preceded by an adjective, and followed by any word form of lemma “be.”\n",
    "\n",
    "![spaCy's rule-based matcher](Images/rulebasedmatcher.jpg)\n",
    "\n",
    "Even when we’re building ML-based models, we can use such heuristics to handle special cases—for example, cases where the model has failed to learn well.\n",
    "\n",
    "### Building Your Model\n",
    "\n",
    "Further, as we collect more data, our ML model starts beating pure heuristics. At that point, a common practice is to combine heuristics directly or indirectly with the ML model. There are two broad ways of\n",
    "doing that:\n",
    "\n",
    "1. **Create a feature from the heuristic for your ML model** : When there are many heuristics where the behavior of a single heuristic is deterministic but their combined behavior is fuzzy in terms of how they predict, it’s best to use these heuristics as features to train your ML model.\n",
    "\n",
    "    **For instance, in the email spam-classification example, we can add features, such as the number of words from the blacklist in a given email or the email bounce rate, to the ML model.**\n",
    "\n",
    "2. **Pre-process your input to the ML model** : If the heuristic has a really high prediction for a particular kind of class, then it’s best to use it before feeding the data in your ML model.\n",
    "\n",
    "    **For instance, if for certain words in an email, there’s a 99% chance that it’s spam, then it’s best to classify that email as spam instead of sending it to an ML model.**\n",
    "\n",
    "NLP providers provide off-the-shelf APIs to solve various NLP tasks. Once you’re comfortable that the task is feasible and conclude that the off-the-shelf models give reasonable results, you can move toward building custom ML models and improving them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building THE Model\n",
    "\n",
    "We start with a baseline approach and work toward improving it.\n",
    "\n",
    "1. Ensemble and stacking : a common practice is not to have a single model, but to use a collection of ML models.\n",
    "\n",
    "    a) Model stacking : we can feed one model’s output as input for another model\n",
    "    \n",
    "    b) Ensembe : pool predictions from multiple models and make a final prediction.\n",
    "\n",
    "![Model ensemble and stacking](Images/modelensemble_stacking.jpg)\n",
    "\n",
    "**For example, in the email spamclassification case, we can assume that we run three different models: a heuristicbased score, Naive Bayes, and LSTM. The output of these three models is then fed into the meta-model based on logistic regression, which then gives the chances of the email being spam or not.**\n",
    "\n",
    "2. Better feature engineering : Feature selecting, ...\n",
    "\n",
    "3. Transfer learning : Transfer learning tries to transfer preexisting knowledge from a big, well-trained model to a newer model at its initial phase.\n",
    "\n",
    "    As an example, for email spam classification, we can use BERT to fine-tune the email dataset.\n",
    "\n",
    "4. Reapplying heuristics : It’s possible to revisit these cases again at the end of the modeling pipeline to find any common pattern in errors and use heuristics to correct them. We can also apply domain-specific knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Success in this phase depends on two factors:\n",
    "\n",
    "1. Using the right metric for evaluation\n",
    "\n",
    "    * They can also vary depending on the phase: the model building, deployment, and production phases.\n",
    "\n",
    "        Whereas in the first two phases, we typically use ML metrics, in the final phase, we also include business metrics to measure business impact.\n",
    "\n",
    "    * Also, evaluations are of two types: intrinsic and extrinsic.\n",
    "    \n",
    "        * Intrinsic focuses on intermediary objectives,\n",
    "        * Extrinsic focuses on evaluating performance on the final objective.\n",
    "        \n",
    "        For example, consider a spam-classification system.\n",
    "        \n",
    "        * The ML metric will be precision and recall,\n",
    "        * The business metric will be “the amount of time users spent on a spam email.   \n",
    "\n",
    "2. Following the right evaluation process\n",
    "\n",
    "\n",
    "### Intrinsic Evaluation\n",
    "\n",
    "The output of the NLP model on a data point is compared against the corresponding label for that data point, and metrics are calculated based on the match.\n",
    "\n",
    "![Metrics](Images/metrics.jpg)\n",
    "![Metrics](Images/metrics2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**: It allows us to inspect the actual and predicted output for different\n",
    "classes in the dataset\n",
    "\n",
    "**Recall at various ranks**: For example, for information retrieval, a common metric is “Recall at rank K”; it looks for the presence of ground truth in top K retrieved results.\n",
    "\n",
    "In tasks such as translation, automated evaluation may not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extrinsic Evaluation**: \n",
    "\n",
    "Important:\n",
    "\n",
    "* Consider a scenario where the regression model does well on the ML metrics but doesn’t really save a lot of time for the email service users\n",
    "\n",
    "* question-answering model does very well on intrinsic metrics but fails to address a large number of questions\n",
    "\n",
    "> The way to carry out extrinsic evaluation is to set up the business metrics and the process to measure them correctly at the start of the project\n",
    "\n",
    "Intrinsic evaluation can be done mostly by the AI team itself. This makes extrinsic evaluation a much more expensive process as compared to intrinsic evaluation.\n",
    "\n",
    "Bad results in intrinsic evaluation often imply bad results in extrinsic evaluation. However, the converse may not be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Modeling Phases\n",
    "\n",
    "### Deployment\n",
    "\n",
    "Deployment entails plugging the NLP module into the broader system. It involves **making sure input and output data pipelines are in order**, as well as making sure our **NLP module is scalable under heavy load**.\n",
    "\n",
    "### Monitoring\n",
    "We need to ensure that the outputs produced by our models daily make sense.\n",
    "\n",
    "### Model Updating\n",
    "\n",
    "* More training data is generated post-deployment : Once deployed, extracted signals can be used to automatically improve the model\n",
    "\n",
    "    Can also try online learning to train the model automatically on a daily basis.\n",
    "\n",
    "* Training data is not generated post-deployment : Manual labeling could be done to improve evaluation and the models.\n",
    "\n",
    "* Low model latency is required, or model has to be online with near-real-time response :\n",
    "\n",
    "    * Models that can be inferred quickly\n",
    "    * Create memorization strategies like caching\n",
    "    * bigger computing power\n",
    "\n",
    "* Low model latency is not required, or model can be run in an offline fashion : can use more advanced and slower models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
