{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the book\n",
    "\n",
    "\n",
    "![Structure](Images/Structure.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 : NLP - A Primer\n",
    "\n",
    "## NLP Tasks\n",
    "![NLP tasks](Images/NLPTasks.jpg)\n",
    "\n",
    "\n",
    "* **Language modeling** : Learn the probability of a sequence of words appearing in a given language (speech recognition, OCR, ...)\n",
    "\n",
    "* **Text classification** : Bucketing the text into a known set of categories bases on its content.\n",
    "\n",
    "* **Information extraction** : Extracting relevant information from text.\n",
    "\n",
    "* **Information retrieval** : Finding documents relevant.\n",
    "\n",
    "* **Conversational agent** : Building dialogue systems\n",
    "\n",
    "* **Text summarization** : Create short summaries of longer documents\n",
    "\n",
    "* **Question answering** : Building  a system that can automatically answer questions.\n",
    "\n",
    "* **Machine translation** : Converting a piece of text from one language to another.\n",
    "\n",
    "* **Topic modeling** : Uncovering the topical structure of a large collection of documents.\n",
    "\n",
    "\n",
    "## Language building blocks\n",
    "\n",
    "We can think of human language as composed of four major building blocks: phonemes,\n",
    "morphemes and lexemes, syntax, and context. NLP applications need knowledge\n",
    "of different levels of these building blocks, starting from the basic sounds\n",
    "of language (phonemes) to texts with some meaningful expressions (context).\n",
    "\n",
    "![NLP tasks](Images/BuildingBlocksLanguage.jpg)\n",
    "\n",
    "* **Phonemes** : Smallest unit of sound in a language. Standard English has 44 phonemes. -> Speech understanding.\n",
    "\n",
    "* **Morphemes and lexemes** :\n",
    "\n",
    "    - Morphemes = Smallest unit of language that has meaning. All prefixes and suffixes are morphemes. Ex: Un + Break + Able\n",
    "\n",
    "    - Lexemes : structural variations of morphemes related to one another by meaning.\n",
    "For example, “run” and “running” belong to the same lexeme form.\n",
    "\n",
    "    --> Tokenization, Stemming, Speech tagging\n",
    "\n",
    "* **Syntax** : Set of rules to construct grammatically correct sentences out of words and phrases in a language. This has a hierarchical structure of language, with words at the lowest level, followed by part-of-speech tags, followed by phrases, and ending with a sentence at the highest level. In Figure 1-6, both sentences have a similar structure and hence a similar syntactic\n",
    "\n",
    "* **Context** : how various parts in a language come together to convey a particular\n",
    "meaning. Long term references, world knowledge, common sense, literal meaning, ... -> Sarcasm detection, summarization and topic modeling.\n",
    "\n",
    "\n",
    "## NLP Challenges\n",
    "\n",
    "* Ambiguity\n",
    "\n",
    "* Common knowledge : It is the set of all facts that most humans are aware of. In any conversation, it is assumed that these facts are known.\n",
    "\n",
    "* Creativity : Various styles, dialects, genres, and variations.\n",
    "\n",
    "* Diversity across languages : A solution that works for one language might not work at all for\n",
    "another language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to NLP\n",
    "\n",
    "### Heuristics-Based NLP\n",
    "\n",
    "Similar to other early AI systems, early attempts at designing NLP systems were based\n",
    "on building rules for the task at hand. This required that the developers had some\n",
    "expertise in the domain to formulate rules that could be incorporated into a program.\n",
    "\n",
    "***Example :***\n",
    " * ***Lexicon-based sentiment analysis***\n",
    " \n",
    " * ***Wordnet, a database of words and the semantic relationships between them*** (synonymes, hyponyms, meronyms)\n",
    "\n",
    " * ***Common sense world knowledge : Open Mind Common sense***\n",
    "\n",
    " * ***Regex***\n",
    " \n",
    " * ***Context-free grammar (CFG) : type of formal grammar that is used to model natural\n",
    "languages. To capture more complex and hierarchical information that a regex might not. -> For rules bases systems like GATE (text extraction for closed and well-defined domains).***\n",
    "\n",
    "### Machine learning for NLP\n",
    "\n",
    "NLP Classification to classify news to a topic, estiamte the price of a stock based on social media discussion.\n",
    "\n",
    "* ***Naive Bayes*** : Naive Bayes is a classic algorithm for classification tasks that mainly relies on Bayes’ theorem (as is evident from the name). Using Bayes’ theorem, it calculates the probability of observing a class label given the set of features for the input data. A characteristic of this algorithm is that it assumes each feature is independent of all\n",
    "other features.\n",
    "\n",
    "* ***Support vector machine*** : The support vector machine (SVM) is another popular classification algorithm. The goal in any classification approach is to learn a decision boundary that acts as a separation between different categories of text (e.g., politics versus sports in our news classification example). This decision boundary can be linear or nonlinear (e.g., a circle).\n",
    "\n",
    "* ***Hidden Markov Model*** : statistical model that assumes there is an underlying, unobservable process with hidden states that generates the data—i.e., we can only observe the data once it is generated. An HMM then tries to model the hidden states from this data.\n",
    "\n",
    "For example, consider the NLP task of part-of-speech (POS) tagging, which deals with assigning part-of-speech tags to sentences. HMMs are used for POS tagging of text data. Here, we assume that the text is generated according to an underlying grammar, which is hidden underneath the text. Along with this, HMMs also make the Markov assumption, which means that each hidden state is dependent on the previous state(s).\n",
    "\n",
    "* ***Conditional random fields (CRF)***: performs a classification task on each element in\n",
    "the sequence.\n",
    "\n",
    "Imagine the same example of POS tagging, where a CRF can tag word by word by classifying them to one of the parts of speech from the pool of all POS tags.\n",
    "\n",
    "### Deep Learning for NLP\n",
    "\n",
    "* ***Recurrent neural network*** : language is inherently sequential. A sentence in any language flows from one direction to another (e.g., English reads from left to right). Thus, a model that can progressively read an input text from one end to another can be very useful for language understanding. Recurrent neural networks (RNNs) are specially designed to keep such sequential processing and learning in mind. RNNs have neural units that are capable of remembering what they have processed so far. This memory is temporal, and the information is stored and updated with every time step as the RNN reads the next word in the input.\n",
    "\n",
    "* ***Long short-term memory*** : Despite their capability and versatility, RNNs suffer from the problem of forgetful memory—they cannot remember longer contexts and therefore do not perform well when the input text is long, which is typically the case with text inputs. Long shortterm\n",
    "memory networks (LSTMs), a type of RNN, were invented to mitigate this shortcoming of the RNNs. LSTMs circumvent this problem by letting go of the irrelevant context and only remembering the part of the context that is needed to solve the task at hand. This relieves the load of remembering very long context in one vector representation.\n",
    "Gated recurrent units (GRUs) are another variant of RNNs that are used mostly in language generation.\n",
    "\n",
    "![Structure](Images/LTSM.jpg)\n",
    "\n",
    "* ***Convolutional neural networks*** :\n",
    "CNNs have also seen success in NLP, especially in text-classification tasks. One can replace each word in a sentence with its corresponding word vector, and all vectors are of the same size (d) (refer to “Word Embeddings” in Chapter 3). Thus, they can be stacked one over another to form a matrix or 2D array of dimension n ✕ d, where n is the number of words in the sentence and d is the size of the word vectors. This matrix can now be treated similar to an image and can be modeled by a CNN. The main advantage CNNs have is their ability to look at a group of words together using a context window.\n",
    "\n",
    "![Structure](Images/CNN.jpg)\n",
    "\n",
    "* ***Transformers*** : \n",
    "\n",
    "They model the textual context but not in a sequential manner.Given a word in the input, it prefers to look at all the words around it (known as selfattention) and represent each word with respect to its context. For example, the word “bank” can have different meanings depending on the context in which it appears. If the context talks about finance, then “bank” probably denotes a financial institution.\n",
    "On the other hand, if the context mentions a river, then it probably indicates a bank of the river.\n",
    "\n",
    "Recently, large transformers have been used for transfer learning with smaller downstream tasks. Transfer learning is a technique in AI where the knowledge gained while solving one problem is applied to a different but related problem. With transformers, the idea is to train a very large transformer mode in an unsupervised manner (known as pre-training) to predict a part of a sentence given the rest of the content so that it can encode the high-level nuances of the language in it. \n",
    "\n",
    "Exemple : BERT\n",
    "\n",
    "![Structure](Images/BERT.jpg)\n",
    "\n",
    "* ***Autoencorders*** : \n",
    "\n",
    "a different kind of network that is used mainly for learning compressed vector representation of the input. For example, if we want to represent a text by a vector, what is a good way to do it? We can learn a mapping function from input text to the vector. To make this mapping function useful, we “reconstruct” the input back from the vector representation. This is a form of unsupervised learning since you don’t need human-annotated labels for it.\n",
    "\n",
    "![Structure](Images/Autoencoder.jpg)\n",
    "\n",
    "Problems:\n",
    "\n",
    "* Overfitting on small datasets\n",
    "\n",
    "* Few-shot learning and synthetic data generation\n",
    "\n",
    "* Domain adaptation : If we utilize a large DL model that is trained on datasets originating from some common domains (e.g., news articles) and apply the trained model to a newer domain that is different from the common domains (e.g., social media posts), it may yield poor performance.\n",
    "\n",
    "* Interpretable models\n",
    "\n",
    "* Common sense and world knowledge\n",
    "\n",
    "* Cost\n",
    "\n",
    "* On-device deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
